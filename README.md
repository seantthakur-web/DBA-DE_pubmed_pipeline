# ğŸ§  DBA-DE PubMed Pipeline

**AI-driven Data Engineering pipeline built during the DBA âœ Data Engineer transition.**

This project demonstrates an end-to-end data pipeline that ingests PubMed research papers, streams them through Kafka, processes them with PySpark, and prepares them for loading into a Neo4j knowledge graph.  
The system simulates how **streaming biomedical data** can be transformed into **graph-based insights** for future **RAG (Retrieval-Augmented Generation)** applications.

---

## ğŸš€ Project Overview

| Layer | Component | Description |
|-------|------------|-------------|
| **1ï¸âƒ£ Ingestion** | `ingestion/pull_pubmed.py` | Queries PubMed (e.g., â€œgastric cancer AND nutritionâ€), saves abstracts locally. |
| **2ï¸âƒ£ Streaming (Kafka)** | `streaming/kafka_producer_stub.py` | Simulates real-time streaming of PubMed abstracts as Kafka messages. |
| **3ï¸âƒ£ ETL (PySpark)** | `streaming/pyspark/transformers/` | Cleans and transforms text before graph loading. |
| **4ï¸âƒ£ Graph Loader (Neo4j)** | `graph/neo4j_loader.py` *(planned)* | Loads 646 biomedical entities and 20 papers into Neo4j for visualization. |
| **5ï¸âƒ£ RAG Layer** | *(planned)* | Enables LLM-powered querying on top of the knowledge graph. |

---

## ğŸ§© Architecture Diagram (Conceptual)

