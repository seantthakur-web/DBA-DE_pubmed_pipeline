# ğŸ§  PubMed Knowledge Graph Pipeline

This repository demonstrates an end-to-end AI Data Engineering pipeline that ingests PubMed research papers, applies NLP entity extraction, performs ETL transformations, and prepares the data for graph loading (Neo4j) and analytics.

---

## ğŸš€ Project Overview

| Layer | Component | Description |
|-------|------------|-------------|
| **1ï¸âƒ£ Ingestion** | `ingestion/pubmed_fetch.py` | Queries PubMed (e.g., â€œgastric cancer AND nutritionâ€), saves abstracts locally. |
| **2ï¸âƒ£ Streaming (Kafka Stub)** | `streaming/kafka_producer_stub.py` | Simulates real-time PubMed messages written to `/data/streaming/`. |
| **3ï¸âƒ£ NLP Extraction** | `nlp_extraction/nlp_extractor.py` | Uses BioBERT (`d4data/biomedical-ner-all`) to extract biomedical entities. |
| **4ï¸âƒ£ ETL Transformation** | `etl/etl_transform.py` | Cleans, structures, and stores extracted data into normalized tables. |
| **5ï¸âƒ£ Storage / Graph Prep** | `data/storage/pubmed_etl.db` | SQLite DB ready for Neo4j loading and downstream analytics. |

---

## ğŸ“‚ Directory Structure

data/
â”£ raw_pubmed/ # Raw PubMed XML/JSON downloads
â”£ streaming/ # Simulated Kafka JSONs
â”£ extracted/ # BioBERT entity extraction output
â”— storage/ # Final ETL output (SQLite DB)
etl/
â”£ etl_transform.py
â”£ verify_etl.py
streaming/
â”— kafka_producer_stub.py
ingestion/
â”— pubmed_fetch.py
nlp_extraction/
â”— nlp_extractor.py



---

## ğŸ§© ETL Processing (Transformation Layer)

This layer structures extracted PubMed abstracts into relational tables ready for Neo4j ingestion.

### ğŸ“‚ Input
`/data/extracted/nlp_extracted_*.json` â€” entity-annotated data from the BioBERT extraction layer.

### ğŸ“¤ Output
`/data/storage/pubmed_etl.db` â€” SQLite database containing normalized tables:

| Table | Description | Example Count |
|--------|--------------|----------------|
| `papers` | PubMed abstracts with titles, abstracts, and PMIDs | 20 |
| `entities` | Extracted biomedical entities (name, type, entity_id) | 646 |
| `relations` | Placeholder for future relation extraction | 0 |

### ğŸ§± Schema Overview

**papers**

| Column | Type | Description |
|---------|------|-------------|
| pmid | String | PubMed Identifier |
| title | String | Cleaned article title |
| abstract | String | Cleaned abstract text |

**entities**

| Column | Type | Description |
|---------|------|-------------|
| entity_id | String | Unique identifier (UUID) |
| name | String | Extracted entity token |
| type | String | Entity category (e.g., Medication, Symptom, Therapeutic_procedure) |

**relations**

| Column | Type | Description |
|---------|------|-------------|
| subject_id | String | Entity acting as subject |
| object_id | String | Entity acting as object |
| relation_type | String | Relation label (to be populated later) |

---

### âš™ï¸ Run Instructions

```bash
# Activate virtual environment
.\.venv\Scripts\activate

# Execute full pipeline
.\run_pipeline.ps1

After successful execution:
âœ… All layers completed successfully!
ğŸ“Š Check your results in: data/storage/pubmed_etl.db

ğŸ§¾ Verification Example

import sqlite3, pandas as pd
conn = sqlite3.connect("data/storage/pubmed_etl.db")

print(pd.read_sql_query("SELECT * FROM papers LIMIT 3;", conn))
print(pd.read_sql_query("SELECT * FROM entities LIMIT 3;", conn))
print(pd.read_sql_query("SELECT COUNT(*) FROM relations;", conn))

conn.close()


ğŸ§  Notes

Week 7 output format: SQLite (.db) for validation and debugging.

Week 8 will evolve to Parquet using PySpark for dbt/Airflow optimization.

The relations table is reserved for future expansion of entity relationships in Neo4j.

âœ… Current Status
Sprint	Focus	Status
        Python   ETL + Environment Setup	âœ… Done (This was done prior to added to innovation sprints)
Sprint 1	PySpark Transformation + Entity Extraction	âœ… Done
Sprint 2	 Airflow / dbt Integration	ğŸš§ Upcoming (starts in october 22nd)


## ğŸ§© Next Steps

1. **Transition ETL Output to Parquet Format (Week 8)**  
   Convert current SQLite output (`pubmed_etl.db`) to columnar **Parquet** format using PySpark to support dbt modeling and downstream analytics.

2. **Create Airflow DAG for Full Orchestration**  
   Build an Airflow workflow that automates the PubMed pipeline from ingestion â†’ NLP â†’ ETL â†’ storage, ensuring end-to-end scheduling and monitoring.

3. **Develop Neo4j Loader for Graph Population**  
   Design Cypher scripts and a loader module to transform the structured data (papers â†’ entities â†’ relations) into a Neo4j graph database.

4. **Integrate LangGraph / RAG Pipeline for AI Retrieval**  
   Extend the system with **LangGraph** to enable retrieval-augmented generation (RAG) over biomedical graph data, improving context-aware question answering.

Author: Sean T.
Maintainer: Data Engineering Track (DBA â†’ DE Transition) with AI layers :)




